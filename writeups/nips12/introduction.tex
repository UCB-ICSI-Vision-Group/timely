\section{Introduction}

In recent years, the computer vision field has converged on a general method for object detection, and a standard evaluation of results.
In the evaluation ground truth, each object is assumed to belong to one of a fixed set of classes; its location is approximated by placing a bounding box around pixels belonging to it.
Large datasets of such human annotations are used for evaluation of detection algorithms~\cite{pascal-voc-2010,imagenet_cvpr09}.

With few exceptions, current detection systems are not inherently multi-class.
Instead, separate detectors are trained per class, and they are deployed and evaluated independently.
The evaluation of multi-class performance consists of averaging the per-class metrics.
State-of-the-art detection systems also do not generally make efficiency a goal.

\begin{figure}[ht!]
\center{\includegraphics[width=0.56\linewidth]
    {../figures/evaluation_thumbs.png}}
  \caption{We aim for \emph{Anytime} performance within the bounds of the curve. That is, the policy should give the best possible answer at any time from start time $T_s$ to deadline $T_d$.}
  \label{fig:evaluation}
\end{figure}

There are of course many applications of object recogniition for which performance truly is time-sensitive.
In robotics, a small finite amount of processing power per unit time is all that is available for robust object detection if the robot is to usefully interact with humans.
In large-scale detection system deployments, such as for image search, results need to be obtained quickly per image as the number of images to process is large and growing.
When processing large photo collection on end-user machines for immediate consumer navigation, the same is true.

In all these cases, an acceptable answer at a reasonable time may be more valuable than the best answer given too late.
Furthermore, the value of the answer depends largely on the target application.

A hypothetical recognition system for a vision-based advertising deployment presents a case study.
The system will have different accuracies for objects of different classes; detections will have different values based on confidence and class; and the queue of unprocessed images will vary in size.
The most rational detection strategy in such an environment should depend on all of these variables.

We argue that the key to tackling such problems of dynamic recognition resource allocation is to start asking a new question:
\emph{What is the best performance we can get on a budget?}
To answer it, we consider the evaluation metric of performance vs. time, shown in Figure~\ref{fig:evaluation} and discussed further in the text.

Our system takes various detectors and classifiers as black boxes, and derives a dynamic policy for selecting detector and other actions to achieve the highest performance under this evaluation.
