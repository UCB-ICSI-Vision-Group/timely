\section{Recognition Problems and Related Work}

Formally, we deal with a dataset of images $\mathcal{D}$, where each image $\mathcal{I}$ contains zero or more objects.
Each object is labeled with exactly one category label $k \in \{1, \dots, K\}$.

The multi-class, multi-label \textbf{classification} problem asks whether $\mathcal{I}$ contains at least one object of class $k$.
We write the ground truth for an image as $\mathbf{C}=\{C_1,\dots,C_K\}$, where $C_k \in \{0,1\}$ is set to $1$ if an object of class $k$ is present.

The \textbf{detection} problem is to output a list of bounding boxes (sub-images defined by four coordinates), each with a real-valued confidence that it encloses a single instance of an object of class $k$, for each $k$.
The answer for a single class $k$ is given by an algorithm $\emph{detect}(\mathcal{I},k)$, which outputs a list of sub-image bounding boxes $B$ and their associated confidences.

Performance is evaluated by plotting precision vs. recall across dataset $\mathcal{D}$ (by progressively lowering the confidence threshold for a positive detection).
The area under the curve yields the Average Precision (AP) metric, which has become the standard evaluation for recognition performance on challenging datasets in vision \cite{pascal-voc-2010}.
A common measure of a correct detection is the PASCAL overlap: two bounding boxes are considered to match if they have the same class label and the ratio of their intersection to their union is at least $\frac{1}{2}$.

To highlight the hierarchical structure of these problems, we note that the confidences for each sub-image $b \in B$ may be given by $\emph{classify}(b,k)$, and, more saliently for our setup, correct answer to the detection problem also answers the classification problem.

Multi-class performance is evaluated by averaging the individual per-class AP values.
In a specialized system such as the advertising case study from~\autoref{sec:introduction}, the metric generalizes to a weighted average, with the weights set by the \emph{values} of the classes.

\subsection{Related Work}

\myparagraph{Object detection}
The best recent performance has come from detectors that use gradient-based features to represent objects as either a collection of local patches or as object-sized windows \cite{Dalal2005,Lowe2004}.
Classifiers are then used to distinguish between featurizations of a given class and all other possible contents of an image window.
Window proposal is most often done exhaustively over the image space, as a ``sliding window''.

For state-of-the-art performance, the object-sized window models are augmented with parts \cite{Felzenszwalb2010a}, and the bag-of-visual-words models employ non-linear classifiers \cite{Vedaldi2009}.
We employ the widely used Deformable Part Model detector \cite{Felzenszwalb2010a} in our evaluation.
%Some approaches use ``jump windows'' (hypotheses voted on by local features) \cite{Vedaldi2009,Vijayanarasimhan2011}, or a bounded search over the space of all possible windows \cite{Lampert2008a}.

%None of the best-performing systems treat window proposal and evaluation as a closed-loop system, with feedback from evaluation to proposal.
%Some work has been done on this topic, mostly inspired by ideas from biological vision and attention research~\cite{Butko2009,Vogel2008}.

% Most detection methods train individual models for each class.
% Work on inherently multi-class detection focuses largely on making detection time sublinear in the number of classes through sharing features \cite{Torralba2007,Fan2005}.

% A post-processing extension to detection systems uses structured prediction to incorporate multi-class context as a principled replacement for non-maximum suppression \cite{Desai2009}.

\myparagraph{Using context}
The most common source of context for detection is the \emph{scene} or other non-detector cues; the most common scene-level feature is the GIST \cite{Oliva2001a} of the image.
We use this source of scene context in our evaluation.

Inter-object context has also been shown to improve detection \cite{Torralba2004}.
In a standard evaluation setup, inter-object context plays a role only in post-filtering, once all detectors have been run.
In contrast, our work leverages inter-object context in the action-planning loop.

A critical summary of the main approaches to using context for object and scene recognition is given in \cite{Galleguillos2010}.
For the commonly used PASCAL VOC dataset \cite{pascal-voc-2010}, GIST and other sources of context are quantitatively explored in~\cite{Divvala2009}.

\myparagraph{Efficiency through cascades}
An early success in efficient object detection of a single class uses simple, fast features to build up a \emph{cascade} of classifiers, which then considers image regions in a sliding window regime \cite{Viola2001}.
Most recently, cyclic optimization has been applied to optimize cascades with respect to feature computation cost as well as classifier performance \cite{Chen2012}.

Cascades are not dynamic policies: they cannot change the order of execution based on observations obtained during execution, which is our goal.

\myparagraph{Anytime and active classification}
This surprisingly little-explored line of work in vision is closest to our approach.
A recent application to the problem of visual detection picks features with maximum value of information in a Hough-voting framework \cite{Vijayanarasimhan2010}. 
There has also been work on active classification \cite{Gao2011} and active sensing \cite{Yu2009}, in which intermediate results are considered in order to decide on the next classification step.
Most commonly, the scheduling in these approaches is greedy with respect to some manual quantity such as expected information gain.
In contrast, we learn policies that take actions without any immediate reward.
