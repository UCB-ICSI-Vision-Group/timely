Edit Tasks
---

- [x] add time arrow to figure 1x

- [x] Make it clear what an intermediate state of the system consists of: per-class detections, or per-region detections?
  - [] but mention that other forms of incompleteness can be incorporated into framework

- Clarify the reward function definition:
  - the actual reward of an action is the expected *final* AP vs. Time evaluation. But delaying the reward until the end of the episode would result in less stable learning, so we formulate a way to calculate piecewise rewards per action. Maybe mention reward scaling from Russell and Ng (although I don't think it applies).
  - [] it's fine if it decreases (see rebuttal), and it does depend on deadline, because of the specific case of the red line: ref. figure
  - [x] Correct ap^j in equation (2)

- [x] Make the RL formalism more accessible to a non-RL person
  - [x] clarify what is meant by discount

- [-] clarify what is meant by the entropy feature: that classes are not mutually exclusive, and this is entropy per class

- [x] Clarify what "values of classes" could correspond to in a "real system" in the intro

- [x] talk about training time of the system

- [] MRF doesn't actually contribute much to results: should mention that?

- [] Mention caching of frequent cases for inference for extra speed
  - probably bad idea because draws attention to cost of inference, which we don't discuss elsewhere

Code Tasks
---

- Use products of class posteriors as additional features to be able to learn more effective correlations
- One reviewer correctly noted that in comparing against baselines, we ignore the MRF inference cost

High-level Comments
---

- The paper can have larger impact if the reinforcement learning formalism is made even more accessible for the vision community. 

- On page 3, it is not clear yet why works using context are relevant to this paper. Perhaps a summary of the proposed approach, or examples of actions the systems can take would be useful. Even till the latter part of page 3, it is not clear what exactly is the desired output of the system (detections or classifications), what are example actions the system can take, etc. 

- It would be nice if the authors made it clear early on what an incomplete response consists of. Does it mean an approximate set of detection scores on the entire image? Or does it mean a subset of detector results? On reading the paper it is clear that it is the latter, but in the abstract, intro, etc. it is not clear along which dimension the output of the system is incomplete if stopped at any point in time. On the same note, can other forms of "incompleteness" be incorporated in this framework? 

- My main doubt about this paper concerns the definition of the reward in equation (2). 
  If we refer to Figure 3b, then the reward is the trapezoid which is *on the right* of the yellow line up to time Td. 
  This definition of the reward depends on Td while the goal is to have the best possible result at any point in time (independently of Td). 
  Also, this definition implicitly assumes that the AP vs Time curve is non-decreasing. Is this guaranteed? 
  I do not undertand why the authors did not rather consider the trapezoid which is *below* the yellow line down to AP=0. 
  This choice would circumvent the two problems mentioned above. 

- It is proposed to include in the feature phi(s,a) the 'entropies of all classes' (line 252). 
I understand what would be the entropy of a set of classes: if we denote p_k = p(C_k|o) then this would be - \sum_k p_k log(p_k). 
However, I do not understand how you compute *one entropy per class*. 
Do you mean that for each class you compute -p_k log(p_k) - (1-p_k) log(1-p_k) ? 
This should be clarified. 

- It is stated line 292 that the action value function can 'learn correlations between presence if different classes'. 
However, a linear function is learned (line 193.5) which only enables to learn a very weak correlation between the classes. 
Given the small number of classes considered, have the authors considered products of class posteriors in their feature phi(s,a) 
to learn stronger correlation relationships? 

- It is stated that the GIST classification action 'brings another boost in performance' (line 370). 
I find the term 'boost' grossly overstated given the very modest improvement it brings: +0.04 for (0.20), +0.01 for (0.10) and +0.02 for (5,15). 

- I am very surprised to read that the GIST action takes as much as 0.3s per image. 
Indeed, the GIST is a very simple feature which is (i) very cheap to compute and (ii) very cheap to classifiy given its low dimensionality. 
Would the authors please elaborate on this aspect? What kind of implementation of the GIST did they use? 

- Could the authors also provide more details about the cost of the part-model detector of [27], 
especially with respect to the GIST classification action? 

- The application of reinforcement learning in computer vision is definitely new, but the proposed technique is not very convincing. The paper only compared with some extremely simple baselines. The closed work to this paper is [18], at the very least, you should have compared with [18]. 

- The comparison in Table 1 is not fair to the baselines. For RL with MRF, you need to do a loopy belief propagation, but the baselines do not. So the time bounds for different methods in Table 1 are tricky. It is not entirely clear whether they are comparable. 

- The paper also shows some graphs in Fig 1 and Fig 2. But I could not see what information they provide. What exactly am I supposed to be looking at in the figures? 

- 